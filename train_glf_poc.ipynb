{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tUser Profiles: Gather user demographic information, preferences, and behavior data.\n",
    "# •\tItem Features: Collect detailed attributes of the items (e.g., products, articles) you want to recommend.\n",
    "# •\tInteraction History: Compile historical interaction data between users and items, including clicks, purchases, ratings, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Number of users and items\n",
    "num_users = 100\n",
    "num_items = 50\n",
    "\n",
    "# Feature dimensions\n",
    "user_feature_dim = 5\n",
    "item_feature_dim = 5\n",
    "\n",
    "# Generate user profiles\n",
    "user_ids = np.arange(num_users)\n",
    "user_features_array = np.random.randn(num_users, user_feature_dim)\n",
    "user_profiles_df = pd.DataFrame(user_features_array, columns=[f'user_feature_{i}' for i in range(user_feature_dim)])\n",
    "user_profiles_df.insert(0, 'user_id', user_ids)\n",
    "\n",
    "# Adjust item IDs to start from 1 to handle padding index 0\n",
    "item_ids = np.arange(1, num_items + 1)  # Now item_ids from 1 to num_items\n",
    "num_items = num_items  # No need to adjust num_items\n",
    "item_features_array = np.random.randn(num_items, item_feature_dim)\n",
    "# Add padding row at index 0\n",
    "padding_item_feature = np.zeros((1, item_feature_dim))\n",
    "item_features_array = np.vstack([padding_item_feature, item_features_array])\n",
    "item_features_df = pd.DataFrame(item_features_array, columns=[f'item_feature_{i}' for i in range(item_feature_dim)])\n",
    "item_ids_with_padding = np.arange(0, num_items + 1)  # From 0 to num_items, where 0 is padding index\n",
    "item_features_df.insert(0, 'item_id', item_ids_with_padding)\n",
    "\n",
    "# Generate interaction history with is_click interaction\n",
    "interaction_records = []\n",
    "for user_id in user_ids:\n",
    "    interacted_items = np.random.choice(item_ids[1:], size=np.random.randint(1, 10), replace=False)\n",
    "    is_clicks = np.random.choice([0, 1], size=len(interacted_items))  # Random is_click values\n",
    "    for item_id, is_click in zip(interacted_items, is_clicks):\n",
    "        interaction_records.append({'user_id': user_id, 'item_id': item_id, 'interaction': 1, 'is_click': is_click})\n",
    "interaction_history_df = pd.DataFrame(interaction_records)\n",
    "\n",
    "# Convert user profiles to tensors\n",
    "user_ids_tensor = torch.tensor(user_profiles_df['user_id'].values, dtype=torch.long)\n",
    "user_features_tensor = torch.tensor(user_profiles_df.drop('user_id', axis=1).values, dtype=torch.float)\n",
    "\n",
    "# Convert item features to tensors\n",
    "item_ids_tensor = torch.tensor(item_features_df['item_id'].values, dtype=torch.long)\n",
    "item_features_tensor = torch.tensor(item_features_df.drop('item_id', axis=1).values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interaction History:\n",
      "   user_id  item_id  interaction  is_click\n",
      "0        0       14            1         1\n",
      "1        0        5            1         1\n",
      "2        0       24            1         1\n",
      "3        0       44            1         1\n",
      "4        0       31            1         0\n"
     ]
    }
   ],
   "source": [
    "# print(\"User Profiles:\")\n",
    "# print(user_profiles_df.head())\n",
    "\n",
    "# print(\"\\nItem Features:\")\n",
    "# print(item_features_df.head())\n",
    "\n",
    "print(\"\\nInteraction History:\")\n",
    "print(interaction_history_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert user profiles to tensors\n",
    "user_ids_tensor = torch.tensor(user_profiles_df['user_id'].values, dtype=torch.long)\n",
    "user_features_tensor = torch.tensor(user_profiles_df.drop('user_id', axis=1).values, dtype=torch.float)\n",
    "\n",
    "# Convert item features to tensors\n",
    "item_ids_tensor = torch.tensor(item_features_df['item_id'].values, dtype=torch.long)\n",
    "item_features_tensor = torch.tensor(item_features_df.drop('item_id', axis=1).values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2341,  1.5792,  0.7674, -0.4695,  0.5426])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features_tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "class DataEnvironment:\n",
    "    def __init__(self, user_features, item_features, interaction_history, slate_size, device):\n",
    "        self.user_features = user_features  # Tensor of shape (num_users, user_feature_dim)\n",
    "        self.item_features = item_features  # Tensor of shape (num_items + 1, item_feature_dim) including padding\n",
    "        self.interaction_history = interaction_history\n",
    "        self.slate_size = slate_size\n",
    "        self.device = device\n",
    "\n",
    "        self.num_users = user_features.shape[0]\n",
    "        self.num_items = item_features.shape[0] - 1  # Exclude padding index\n",
    "        self.user_feature_dim = user_features.shape[1]\n",
    "        self.item_feature_dim = item_features.shape[1]\n",
    "        self.candidate_items = torch.arange(1, self.num_items + 1).to(device)  # From 1 to num_items\n",
    "\n",
    "    def reset(self, batch_size):\n",
    "        # Randomly select users\n",
    "        user_indices = np.random.choice(self.num_users, batch_size, replace=False)\n",
    "        user_profiles = self.user_features[user_indices].to(self.device)\n",
    "\n",
    "        # Prepare user history\n",
    "        user_histories = []\n",
    "        history_lengths = []\n",
    "        max_history_length = 10  # Set a maximum history length\n",
    "        for user_id in user_indices:\n",
    "            # Get user's interaction history\n",
    "            user_history = self.interaction_history[self.interaction_history['user_id'] == user_id]\n",
    "            # Get item_ids and is_click\n",
    "            history_items = user_history['item_id'].values\n",
    "            is_clicks = user_history['is_click'].values\n",
    "            # Limit to max_history_length\n",
    "            if len(history_items) > max_history_length:\n",
    "                history_items = history_items[:max_history_length]\n",
    "                is_clicks = is_clicks[:max_history_length]\n",
    "            history_length = len(history_items)\n",
    "            history_lengths.append(history_length)\n",
    "            # Pad history to max_history_length with padding index 0\n",
    "            padded_history_items = np.pad(history_items, (0, max_history_length - history_length), 'constant', constant_values=0)\n",
    "            padded_is_clicks = np.pad(is_clicks, (0, max_history_length - history_length), 'constant', constant_values=0)\n",
    "            # Store as tensors\n",
    "            user_histories.append((padded_history_items, padded_is_clicks))\n",
    "\n",
    "        # Convert user_histories to tensors\n",
    "        history_items_tensor = torch.tensor([h[0] for h in user_histories], dtype=torch.long).to(self.device)\n",
    "        is_clicks_tensor = torch.tensor([h[1] for h in user_histories], dtype=torch.long).to(self.device)\n",
    "        history_lengths_tensor = torch.tensor(history_lengths, dtype=torch.long).to(self.device)\n",
    "\n",
    "        observation = {\n",
    "            'user_profile': {\n",
    "                'user_id': torch.tensor(user_indices, dtype=torch.long).to(self.device),\n",
    "                'uf_embedding': user_profiles,\n",
    "            },\n",
    "            'user_history': {\n",
    "                'history_items': history_items_tensor,  # Shape: (batch_size, max_history_length)\n",
    "                'is_clicks': is_clicks_tensor,          # Shape: (batch_size, max_history_length)\n",
    "                'history_length': history_lengths_tensor,\n",
    "            },\n",
    "        }\n",
    "        return observation\n",
    "\n",
    "    def get_candidate_info(self):\n",
    "        candidates = {\n",
    "            'item_id': self.candidate_items.view(1, -1),\n",
    "            'item_embedding': self.item_features[1:].view(1, self.num_items, -1).to(self.device),\n",
    "        }\n",
    "        return candidates\n",
    "\n",
    "    def step(self, user_ids, action):\n",
    "        # Simulate user feedback based on interaction history\n",
    "        batch_size = action.shape[0]\n",
    "        reward = torch.zeros(batch_size).to(self.device)\n",
    "        for i in range(batch_size):\n",
    "            user_id = user_ids[i].item()\n",
    "            recommended_items = action[i].cpu().numpy()\n",
    "            # Get items the user has interacted with\n",
    "            user_history = self.interaction_history[self.interaction_history['user_id'] == user_id]\n",
    "            interacted_items = user_history['item_id'].values\n",
    "            # Reward is the fraction of recommended items the user has interacted with\n",
    "            if len(interacted_items) > 0:\n",
    "                reward[i] = np.isin(recommended_items, interacted_items).mean()\n",
    "            else:\n",
    "                reward[i] = 0.0  # No interactions for this user\n",
    "        user_feedback = {\n",
    "            'reward': reward,\n",
    "            'immediate_response': (reward.unsqueeze(1).repeat(1, self.slate_size) > 0).unsqueeze(-1).float(),\n",
    "        }\n",
    "        return user_feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tDaily Recommendations: In a real application, a “step” could correspond to generating recommendations for users each day.\n",
    "# •\tSession-Based Interactions: If users interact with the system multiple times in a session, each “step” could simulate a new interaction within the session.\n",
    "# •\tTime Slots: For systems that update recommendations periodically (e.g., every hour), each “step” could represent a time slot.\n",
    "# Time --->\n",
    "\n",
    "# | Step 1 | Step 2 | Step 3 | ... | Step N |\n",
    "\n",
    "# At each step:\n",
    "\n",
    "# - Select users\n",
    "# - Generate recommendations\n",
    "# - Simulate user feedback\n",
    "# - Collect and analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/3\n",
      "User ID: 88\n",
      "Recommended Slate: [43 18 27 15 27]\n",
      "Reward: 0.20\n",
      "User History Items: [22 43 29 13 24 14  3 46  7  0]\n",
      "User History Clicks: [0 0 0 0 0 0 1 1 0 0]\n",
      "------------------------------\n",
      "User ID: 78\n",
      "Recommended Slate: [36 21 25  1 14]\n",
      "Reward: 0.20\n",
      "User History Items: [14 22 35  3  9 13 15  0  0  0]\n",
      "User History Clicks: [0 0 0 1 0 0 0 0 0 0]\n",
      "------------------------------\n",
      "==================================================\n",
      "Step 2/3\n",
      "User ID: 23\n",
      "Recommended Slate: [29 15 11  5 32]\n",
      "Reward: 0.00\n",
      "User History Items: [48  7 14  2  0  0  0  0  0  0]\n",
      "User History Clicks: [1 1 1 1 0 0 0 0 0 0]\n",
      "------------------------------\n",
      "User ID: 7\n",
      "Recommended Slate: [23 16 46 18  7]\n",
      "Reward: 0.00\n",
      "User History Items: [31 24  0  0  0  0  0  0  0  0]\n",
      "User History Clicks: [1 1 0 0 0 0 0 0 0 0]\n",
      "------------------------------\n",
      "==================================================\n",
      "Step 3/3\n",
      "User ID: 30\n",
      "Recommended Slate: [50 27 24 12 50]\n",
      "Reward: 0.20\n",
      "User History Items: [37  6 47 12 46 34  7 44 38  0]\n",
      "User History Clicks: [0 0 0 0 1 1 1 1 1 0]\n",
      "------------------------------\n",
      "User ID: 18\n",
      "Recommended Slate: [14 42 20 38 20]\n",
      "Reward: 0.00\n",
      "User History Items: [10 37 48 50 15  0  0  0  0  0]\n",
      "User History Clicks: [1 0 0 0 0 0 0 0 0 0]\n",
      "------------------------------\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1674301/2984395016.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724788959220/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  history_items_tensor = torch.tensor([h[0] for h in user_histories], dtype=torch.long).to(self.device)\n"
     ]
    }
   ],
   "source": [
    "# 2. Simulating User Behavior\n",
    "# We’ll create a function that simulates user interactions with the environment over multiple steps.\n",
    "def simulate_user_interactions(env, num_steps, batch_size):\n",
    "    # Initialize lists to store simulation results\n",
    "    all_user_ids = []\n",
    "    all_recommended_slates = []\n",
    "    all_rewards = []\n",
    "    all_user_histories = []\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Reset environment and get observation\n",
    "        observation = env.reset(batch_size)\n",
    "        user_ids = observation['user_profile']['user_id']\n",
    "        user_profiles = observation['user_profile']['uf_embedding']\n",
    "        user_history = observation['user_history']\n",
    "\n",
    "        # Get candidate items\n",
    "        candidates = env.get_candidate_info()\n",
    "\n",
    "        # For simplicity, we'll generate random recommendations (in a real scenario, we'd use a policy)\n",
    "        # In Practice: A trained policy (e.g., SlateGFN_DB) would generate these recommendations based on user profiles and histories.\n",
    "\n",
    "        recommended_slates = torch.randint(1, env.num_items + 1, (batch_size, env.slate_size)).to(env.device)\n",
    "\n",
    "        # Simulate user feedback\n",
    "        # env.step(user_ids, recommended_slates) simulates how users react to the recommendations\n",
    "        user_feedback = env.step(user_ids, recommended_slates)\n",
    "        rewards = user_feedback['reward']\n",
    "\n",
    "        # Collect results\n",
    "        all_user_ids.extend(user_ids.cpu().numpy())\n",
    "        all_recommended_slates.extend(recommended_slates.cpu().numpy())\n",
    "        all_rewards.extend(rewards.cpu().numpy())\n",
    "        all_user_histories.append(user_history)\n",
    "\n",
    "        # Print step results\n",
    "        print(f\"Step {step + 1}/{num_steps}\")\n",
    "        for i in range(batch_size):\n",
    "            print(f\"User ID: {user_ids[i].item()}\")\n",
    "            print(f\"Recommended Slate: {recommended_slates[i].cpu().numpy()}\")\n",
    "            print(f\"Reward: {rewards[i].item():.2f}\")\n",
    "            print(f\"User History Items: {user_history['history_items'][i].cpu().numpy()}\")\n",
    "            print(f\"User History Clicks: {user_history['is_clicks'][i].cpu().numpy()}\")\n",
    "            print(\"-\" * 30)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    return {\n",
    "        'user_ids': all_user_ids,\n",
    "        'recommended_slates': all_recommended_slates,\n",
    "        'rewards': all_rewards,\n",
    "        'user_histories': all_user_histories\n",
    "    }\n",
    "\n",
    "#  3. Running the Simulation\n",
    "# Simulation parameters\n",
    "num_steps = 3\n",
    "batch_size = 2  # For simplicity, we'll use a small batch size\n",
    "slate_size = 5\n",
    "# Initialize the environment\n",
    "device = 'cpu'\n",
    "env = DataEnvironment(user_features_tensor, item_features_tensor, interaction_history_df, slate_size, device)\n",
    "\n",
    "simulation_results = simulate_user_interactions(env, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/2\n",
      "User ID: 4\n",
      "Recommended Slate: [38  8 37 34  5]\n",
      "Reward: 1.00\n",
      "User History Items: [38  8 37 34 45 47  5 11 21  0]\n",
      "User History Clicks: [0 0 0 0 1 1 0 0 1 0]\n",
      "------------------------------\n",
      "User ID: 73\n",
      "Recommended Slate: [48  8  4 23 27]\n",
      "Reward: 0.40\n",
      "User History Items: [48  8 26 39  0  0  0  0  0  0]\n",
      "User History Clicks: [0 0 1 1 0 0 0 0 0 0]\n",
      "------------------------------\n",
      "==================================================\n",
      "Step 2/2\n",
      "User ID: 69\n",
      "Recommended Slate: [22  6 28 42 33]\n",
      "Reward: 1.00\n",
      "User History Items: [22  6 28 42 33  0  0  0  0  0]\n",
      "User History Clicks: [0 0 0 0 0 0 0 0 0 0]\n",
      "------------------------------\n",
      "User ID: 59\n",
      "Recommended Slate: [26  3 33  7 10]\n",
      "Reward: 1.00\n",
      "User History Items: [26  3 33  7 18 10  0  0  0  0]\n",
      "User History Clicks: [0 0 0 0 1 0 0 0 0 0]\n",
      "------------------------------\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#  Simple Policy for Recommendations\n",
    "\n",
    "def simple_policy(user_history, num_items, slate_size):\n",
    "    # Recommend items that the user has interacted with but not clicked\n",
    "    batch_size = user_history['history_items'].shape[0]\n",
    "    recommended_slates = torch.zeros(batch_size, slate_size, dtype=torch.long)\n",
    "    for i in range(batch_size):\n",
    "        history_items = user_history['history_items'][i].cpu().numpy()\n",
    "        is_clicks = user_history['is_clicks'][i].cpu().numpy()\n",
    "        # Get items the user interacted with but did not click\n",
    "        candidate_items = history_items[(history_items != 0) & (is_clicks == 0)]\n",
    "        if len(candidate_items) >= slate_size:\n",
    "            recommended_items = candidate_items[:slate_size]\n",
    "        else:\n",
    "            # Fill the rest with random items\n",
    "            num_random_items = slate_size - len(candidate_items)\n",
    "            random_items = np.random.choice(np.setdiff1d(np.arange(1, num_items + 1), history_items), num_random_items, replace=False)\n",
    "            recommended_items = np.concatenate([candidate_items, random_items])\n",
    "        recommended_slates[i] = torch.tensor(recommended_items)\n",
    "    return recommended_slates.to(user_history['history_items'].device)\n",
    "\n",
    "def simulate_with_policy(env, num_steps, batch_size):\n",
    "    for step in range(num_steps):\n",
    "        # Reset environment and get observation\n",
    "        observation = env.reset(batch_size)\n",
    "        user_ids = observation['user_profile']['user_id']\n",
    "        user_profiles = observation['user_profile']['uf_embedding']\n",
    "        user_history = observation['user_history']\n",
    "\n",
    "        # Get candidate items\n",
    "        candidates = env.get_candidate_info()\n",
    "\n",
    "        # Generate recommendations using the simple policy\n",
    "        recommended_slates = simple_policy(user_history, env.num_items, env.slate_size)\n",
    "\n",
    "        # Simulate user feedback\n",
    "        user_feedback = env.step(user_ids, recommended_slates)\n",
    "        rewards = user_feedback['reward']\n",
    "\n",
    "        # Print step results\n",
    "        print(f\"Step {step + 1}/{num_steps}\")\n",
    "        for i in range(batch_size):\n",
    "            print(f\"User ID: {user_ids[i].item()}\")\n",
    "            print(f\"Recommended Slate: {recommended_slates[i].cpu().numpy()}\")\n",
    "            print(f\"Reward: {rewards[i].item():.2f}\")\n",
    "            print(f\"User History Items: {user_history['history_items'][i].cpu().numpy()}\")\n",
    "            print(f\"User History Clicks: {user_history['is_clicks'][i].cpu().numpy()}\")\n",
    "            print(\"-\" * 30)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# •\tThe simple_policy function recommends items that the user has interacted with but did not click.\n",
    "# •\tIf there are not enough such items, it fills the slate with random items not in the user’s history.\n",
    "# •\tThis policy aims to re-engage users with items they showed interest in but did not click on.\n",
    "simulate_with_policy(env, num_steps=2, batch_size=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy\n",
    "class SlateGFN_DB(nn.Module):\n",
    "    def __init__(self, state_dim, enc_dim, slate_size, num_items, device):\n",
    "        super(SlateGFN_DB, self).__init__()\n",
    "        self.enc_dim = enc_dim\n",
    "        self.slate_size = slate_size\n",
    "        self.num_items = num_items\n",
    "        self.device = device\n",
    "\n",
    "        # Item embedding layer (including padding index 0)\n",
    "        self.item_embedding_layer = nn.Embedding(num_items + 1, enc_dim, padding_idx=0)\n",
    "\n",
    "        # Adjust state_dim to include history embedding\n",
    "        self.state_dim = state_dim + enc_dim\n",
    "\n",
    "        # Forward probability network\n",
    "        self.pForwardEncoder = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + self.enc_dim * self.slate_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.enc_dim),\n",
    "            nn.LayerNorm(self.enc_dim)\n",
    "        )\n",
    "\n",
    "        # Flow network\n",
    "        self.logFlow = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + self.enc_dim * self.slate_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        self.gfn_forward_offset = 1.0\n",
    "        self.gfn_reward_smooth = 1.0\n",
    "        self.gfn_Z = 0.0\n",
    "        self.l2_coef = 1e-4\n",
    "\n",
    "    def encode_user_history(self, history_items, is_clicks):\n",
    "        # history_items: Tensor of shape (batch_size, max_history_length)\n",
    "        # is_clicks: Tensor of shape (batch_size, max_history_length)\n",
    "        # Embed the items\n",
    "        item_embeddings = self.item_embedding_layer(history_items)  # Shape: (batch_size, max_history_length, enc_dim)\n",
    "        # Multiply embeddings by is_clicks\n",
    "        is_clicks = is_clicks.float().unsqueeze(-1)  # Shape: (batch_size, max_history_length, 1)\n",
    "        weighted_embeddings = item_embeddings * is_clicks  # Shape: (batch_size, max_history_length, enc_dim)\n",
    "        # Sum over history length\n",
    "        history_embedding = weighted_embeddings.sum(dim=1)  # Shape: (batch_size, enc_dim)\n",
    "        return history_embedding  # Shape: (batch_size, enc_dim)\n",
    "\n",
    "    def forward(self, user_state, candidates, parent_slate, user_history, is_train=True):\n",
    "        history_items = user_history['history_items']  # Shape: (batch_size, max_history_length)\n",
    "        is_clicks = user_history['is_clicks']          # Shape: (batch_size, max_history_length)\n",
    "        history_embedding = self.encode_user_history(history_items, is_clicks)  # Shape: (batch_size, enc_dim)\n",
    "\n",
    "        # Update user_state to include history_embedding\n",
    "        user_state = torch.cat([user_state, history_embedding], dim=1)  # Shape: (batch_size, state_dim + enc_dim)\n",
    "\n",
    "        B = user_state.shape[0]\n",
    "        candidate_item_enc = candidates['item_embedding'].squeeze(0)  # Shape: (num_items, enc_dim)\n",
    "\n",
    "        current_P = torch.zeros(B, self.slate_size).to(self.device)\n",
    "        current_action = torch.zeros(B, self.slate_size, dtype=torch.long).to(self.device)\n",
    "        current_list_emb = torch.zeros(B, self.slate_size, self.enc_dim).to(self.device)\n",
    "        current_flow = torch.zeros(B, self.slate_size + 1).to(self.device)\n",
    "\n",
    "        for i in range(self.slate_size):\n",
    "            current_state = torch.cat((user_state, current_list_emb.view(B, -1)), dim=1)\n",
    "            selection_weight = self.pForwardEncoder(current_state)\n",
    "            score = torch.matmul(selection_weight, candidate_item_enc.t())  # Shape: (B, num_items)\n",
    "            prob = torch.softmax(score, dim=1)\n",
    "\n",
    "            if is_train:\n",
    "                action_at_i = parent_slate[:, i]\n",
    "                current_P[:, i] = prob[torch.arange(B), action_at_i - 1]  # Adjust for index starting from 1\n",
    "                current_list_emb[:, i, :] = candidate_item_enc[action_at_i - 1]\n",
    "                current_flow[:, i] = self.logFlow(current_state).view(-1)\n",
    "                current_action[:, i] = action_at_i\n",
    "            else:\n",
    "                # For simplicity, select the item with highest probability\n",
    "                action_at_i = torch.argmax(prob, dim=1) + 1  # Adjust index to start from 1\n",
    "                current_P[:, i] = prob[torch.arange(B), action_at_i - 1]\n",
    "                current_list_emb[:, i, :] = candidate_item_enc[action_at_i - 1]\n",
    "                current_flow[:, i] = self.logFlow(current_state).view(-1)\n",
    "                current_action[:, i] = action_at_i\n",
    "\n",
    "        # Terminal flow\n",
    "        current_state = torch.cat((user_state, current_list_emb.view(B, -1)), dim=1)\n",
    "        current_flow[:, -1] = self.logFlow(current_state).view(-1)\n",
    "        reg = self.l2_coef * (sum(p.pow(2.0).sum() for p in self.parameters()))\n",
    "\n",
    "        out_dict = {\n",
    "            'prob': current_P,\n",
    "            'action': current_action,\n",
    "            'logF': current_flow,\n",
    "            'reg': reg,\n",
    "        }\n",
    "        return out_dict\n",
    "\n",
    "    def get_loss(self, out_dict, reward):\n",
    "        parent_flow = out_dict['logF'][:, :-1]\n",
    "        current_flow = out_dict['logF'][:, 1:]\n",
    "        log_P = torch.log(out_dict['prob'] + self.gfn_forward_offset)\n",
    "\n",
    "        forward_part = parent_flow + log_P + self.gfn_Z\n",
    "        backward_part = current_flow\n",
    "\n",
    "        DB_loss = torch.mean((forward_part - backward_part).pow(2))\n",
    "        terminal_loss = torch.mean((current_flow[:, -1] + self.gfn_Z - torch.log(reward + self.gfn_reward_smooth)).pow(2))\n",
    "        loss = DB_loss + terminal_loss + out_dict['reg']\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Initialize the Environment and Policy\n",
    "# Set device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "state_dim = user_feature_dim\n",
    "enc_dim = item_feature_dim\n",
    "slate_size = 5\n",
    "batch_size = 32\n",
    "\n",
    "user_features_tensor = user_features_tensor.to(device)\n",
    "item_features_tensor = item_features_tensor.to(device)\n",
    "\n",
    "env = DataEnvironment(user_features_tensor, item_features_tensor, interaction_history_df, slate_size, device)\n",
    "policy = SlateGFN_DB(state_dim, enc_dim, slate_size, num_items, device).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "num_batches = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1581922/2984395016.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724788959220/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  history_items_tensor = torch.tensor([h[0] for h in user_histories], dtype=torch.long).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0814\n",
      "Epoch 2/5, Loss: 0.0554\n",
      "Epoch 3/5, Loss: 0.0496\n",
      "Epoch 4/5, Loss: 0.0472\n",
      "Epoch 5/5, Loss: 0.0454\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "num_batches = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for _ in range(num_batches):\n",
    "        # Reset environment and get observation\n",
    "        observation = env.reset(batch_size)\n",
    "        user_state = observation['user_profile']['uf_embedding']  # Shape: (B, state_dim)\n",
    "        user_ids = observation['user_profile']['user_id']\n",
    "        user_history = observation['user_history']\n",
    "\n",
    "        # Get candidate items\n",
    "        candidates = env.get_candidate_info()\n",
    "\n",
    "        # For training, generate random slates (parent_slate)\n",
    "        parent_slate = torch.randint(1, env.num_items + 1, (batch_size, slate_size)).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out_dict = policy(user_state, candidates, parent_slate, user_history, is_train=True)\n",
    "\n",
    "        # Get reward from environment\n",
    "        user_feedback = env.step(user_ids, parent_slate)\n",
    "        reward = user_feedback['reward']\n",
    "\n",
    "        # Compute loss\n",
    "        loss = policy.get_loss(out_dict, reward + 1e-6)  # Add small value to prevent log(0)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Slate for first user: [34 36 36 36 36]\n",
      "Average Reward: 0.0938\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "policy.eval()\n",
    "with torch.no_grad():\n",
    "    observation = env.reset(batch_size)\n",
    "    user_state = observation['user_profile']['uf_embedding']  # Shape: (B, state_dim)\n",
    "    user_ids = observation['user_profile']['user_id']\n",
    "    user_history = observation['user_history']\n",
    "    candidates = env.get_candidate_info()\n",
    "    out_dict = policy(user_state, candidates, parent_slate=None, user_history=user_history, is_train=False)\n",
    "    recommended_slate = out_dict['action']\n",
    "    print(\"Recommended Slate for first user:\", recommended_slate[0].cpu().numpy())\n",
    "\n",
    "    # Evaluate performance\n",
    "    user_feedback = env.step(user_ids, recommended_slate)\n",
    "    avg_reward = user_feedback['reward'].mean().item()\n",
    "    print(f\"Average Reward: {avg_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simulate Real-Time User Arrivals\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the DataEnvironment class with the added method\n",
    "class DataEnvironment:\n",
    "    def __init__(self, user_features, item_features, interaction_history, slate_size, device):\n",
    "        self.user_features = user_features  # Tensor of shape (num_users, user_feature_dim)\n",
    "        self.item_features = item_features  # Tensor of shape (num_items + 1, item_feature_dim), including padding\n",
    "        self.interaction_history = interaction_history  # DataFrame with 'user_id', 'item_id', 'interaction', 'is_click'\n",
    "        self.slate_size = slate_size\n",
    "        self.device = device\n",
    "\n",
    "        self.num_users = user_features.shape[0]\n",
    "        self.num_items = item_features.shape[0] - 1  # Exclude padding index\n",
    "        self.user_feature_dim = user_features.shape[1]\n",
    "        self.item_feature_dim = item_features.shape[1]\n",
    "        self.candidate_items = torch.arange(1, self.num_items + 1).to(device)  # From 1 to num_items\n",
    "\n",
    "    def get_user_observation(self, user_index):\n",
    "        user_profile = self.user_features[user_index].unsqueeze(0).to(self.device)\n",
    "    \n",
    "        # Prepare user history\n",
    "        max_history_length = 10  # Set a maximum history length\n",
    "        # Get user's interaction history\n",
    "        user_history = self.interaction_history[self.interaction_history['user_id'] == user_index]\n",
    "        # Get item_ids and is_click\n",
    "        history_items = user_history['item_id'].values\n",
    "        is_clicks = user_history['is_click'].values\n",
    "        # Limit to max_history_length\n",
    "        if len(history_items) > max_history_length:\n",
    "            history_items = history_items[:max_history_length]\n",
    "            is_clicks = is_clicks[:max_history_length]\n",
    "        history_length = len(history_items)\n",
    "        # Pad history to max_history_length with padding index 0\n",
    "        padded_history_items = np.pad(history_items, (0, max_history_length - history_length), 'constant', constant_values=0)\n",
    "        padded_is_clicks = np.pad(is_clicks, (0, max_history_length - history_length), 'constant', constant_values=0)\n",
    "        # Convert to tensors\n",
    "        history_items_tensor = torch.tensor([padded_history_items], dtype=torch.long).to(self.device)\n",
    "        is_clicks_tensor = torch.tensor([padded_is_clicks], dtype=torch.long).to(self.device)\n",
    "        history_lengths_tensor = torch.tensor([history_length], dtype=torch.long).to(self.device)\n",
    "    \n",
    "        observation = {\n",
    "            'user_profile': {\n",
    "                'user_id': torch.tensor([user_index], dtype=torch.long).to(self.device),\n",
    "                'uf_embedding': user_profile,\n",
    "            },\n",
    "            'user_history': {\n",
    "                'history_items': history_items_tensor,  # Shape: (1, max_history_length)\n",
    "                'is_clicks': is_clicks_tensor,          # Shape: (1, max_history_length)\n",
    "                'history_length': history_lengths_tensor,\n",
    "            },\n",
    "        }\n",
    "        return observation\n",
    "\n",
    "    def get_candidate_info(self):\n",
    "        candidates = {\n",
    "            'item_id': self.candidate_items.view(1, -1),\n",
    "            'item_embedding': self.item_features[1:].view(1, self.num_items, -1).to(self.device),\n",
    "        }\n",
    "        return candidates\n",
    "\n",
    "    def step(self, user_ids, action):\n",
    "        batch_size = action.shape[0]\n",
    "        reward = torch.zeros(batch_size).to(self.device)\n",
    "        for i in range(batch_size):\n",
    "            user_id = user_ids[i].item()\n",
    "            recommended_items = action[i].cpu().numpy()\n",
    "            # Get items the user has interacted with\n",
    "            user_history = self.interaction_history[self.interaction_history['user_id'] == user_id]\n",
    "            interacted_items = user_history['item_id'].values\n",
    "            # Reward is the fraction of recommended items the user has interacted with\n",
    "            if len(interacted_items) > 0:\n",
    "                reward[i] = np.isin(recommended_items, interacted_items).mean()\n",
    "            else:\n",
    "                reward[i] = 0.0  # No interactions for this user\n",
    "        user_feedback = {\n",
    "            'reward': reward,\n",
    "            'immediate_response': (reward.unsqueeze(1).repeat(1, self.slate_size) > 0).unsqueeze(-1).float(),\n",
    "        }\n",
    "        return user_feedback\n",
    "\n",
    "\n",
    "# Define the process_user function\n",
    "def process_user(env, user_index):\n",
    "    # Get observation for the user\n",
    "    observation = env.get_user_observation(user_index)\n",
    "    user_id = observation['user_profile']['user_id']\n",
    "    user_profile = observation['user_profile']['uf_embedding']\n",
    "    user_history = observation['user_history']\n",
    "    \n",
    "    # Get candidate items\n",
    "    candidates = env.get_candidate_info()\n",
    "    \n",
    "    # Generate recommendations (e.g., using a policy or random for this example)\n",
    "    recommended_slate = np.random.choice(env.candidate_items.cpu().numpy(), env.slate_size, replace=False)\n",
    "    recommended_slate = torch.tensor(recommended_slate, dtype=torch.long).unsqueeze(0).to(env.device)\n",
    "    \n",
    "    # Simulate user feedback\n",
    "    user_feedback = env.step(user_id.unsqueeze(0), recommended_slate)\n",
    "    reward = user_feedback['reward'].item()\n",
    "    \n",
    "    # Print user interaction details\n",
    "    print(f\"User ID: {user_id.item()}\")\n",
    "    print(f\"Recommended Slate: {recommended_slate.cpu().numpy().flatten()}\")\n",
    "    print(f\"Reward: {reward:.2f}\")\n",
    "    print(f\"User History Items: {user_history['history_items'].cpu().numpy().flatten()}\")\n",
    "    print(f\"User History Clicks: {user_history['is_clicks'].cpu().numpy().flatten()}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Define the simulate_user_arrivals function\n",
    "def simulate_user_arrivals(env, total_duration, average_interarrival_time):\n",
    "    start_time = time.time()\n",
    "    next_arrival_time = start_time + np.random.exponential(average_interarrival_time)\n",
    "    user_queue = []\n",
    "    \n",
    "    while time.time() - start_time < total_duration:\n",
    "        current_time = time.time()\n",
    "        if current_time >= next_arrival_time:\n",
    "            # Simulate user arrival\n",
    "            user_index = np.random.choice(env.num_users)\n",
    "            user_queue.append(user_index)\n",
    "            # Schedule next arrival\n",
    "            next_arrival_time = current_time + np.random.exponential(average_interarrival_time)\n",
    "        \n",
    "        if user_queue:\n",
    "            # Process the next user in the queue\n",
    "            user_index = user_queue.pop(0)\n",
    "            process_user(env, user_index)\n",
    "        \n",
    "        # Sleep briefly to prevent tight loop\n",
    "        time.sleep(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID: 29\n",
      "Recommended Slate: [32 27 20 42 50]\n",
      "Reward: 0.40\n",
      "User History Items: [17 37 23 41 42 50 36  7  0  0]\n",
      "User History Clicks: [1 0 1 0 0 1 1 1 0 0]\n",
      "------------------------------\n",
      "User ID: 59\n",
      "Recommended Slate: [29  3 34  2 31]\n",
      "Reward: 0.20\n",
      "User History Items: [26  3 33  7 18 10  0  0  0  0]\n",
      "User History Clicks: [0 0 0 0 1 0 0 0 0 0]\n",
      "------------------------------\n",
      "User ID: 55\n",
      "Recommended Slate: [26 28 44  6 24]\n",
      "Reward: 0.00\n",
      "User History Items: [32  0  0  0  0  0  0  0  0  0]\n",
      "User History Clicks: [0 0 0 0 0 0 0 0 0 0]\n",
      "------------------------------\n",
      "User ID: 44\n",
      "Recommended Slate: [21 18 46 19 37]\n",
      "Reward: 0.20\n",
      "User History Items: [11 21  8 44 36 28 41  0  0  0]\n",
      "User History Clicks: [0 0 1 0 0 1 0 0 0 0]\n",
      "------------------------------\n",
      "User ID: 48\n",
      "Recommended Slate: [24 46  1 38 11]\n",
      "Reward: 0.60\n",
      "User History Items: [24  4 38 47 46 17 31 13 42  0]\n",
      "User History Clicks: [0 0 1 0 1 1 0 1 1 0]\n",
      "------------------------------\n",
      "User ID: 77\n",
      "Recommended Slate: [ 4  2 47 41 30]\n",
      "Reward: 0.20\n",
      "User History Items: [46 22 32  6 49 41 19 40  0  0]\n",
      "User History Clicks: [0 0 0 0 1 1 0 1 0 0]\n",
      "------------------------------\n",
      "User ID: 54\n",
      "Recommended Slate: [28  1 10 31 12]\n",
      "Reward: 0.00\n",
      "User History Items: [16 24 19 37  0  0  0  0  0  0]\n",
      "User History Clicks: [1 0 0 0 0 0 0 0 0 0]\n",
      "------------------------------\n",
      "User ID: 82\n",
      "Recommended Slate: [40 47 49  5  3]\n",
      "Reward: 0.00\n",
      "User History Items: [26 43 35 17  6 36 10 42  0  0]\n",
      "User History Clicks: [0 0 1 0 1 1 0 0 0 0]\n",
      "------------------------------\n",
      "User ID: 72\n",
      "Recommended Slate: [21 40  8  3  9]\n",
      "Reward: 0.20\n",
      "User History Items: [41 29 11 27 38  5  8 49 45  0]\n",
      "User History Clicks: [1 1 1 1 1 0 0 0 1 0]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simulation parameters\n",
    "total_duration = 10  # Total simulation time in seconds\n",
    "average_interarrival_time = 2  # Average time between user arrivals in seconds\n",
    "slate_size = 5\n",
    "\n",
    "# Initialize the environment\n",
    "device = 'cpu'\n",
    "env = DataEnvironment(user_features_tensor, item_features_tensor, interaction_history_df, slate_size, device)\n",
    "\n",
    "# Run the real-time simulation\n",
    "simulate_user_arrivals(env, total_duration, average_interarrival_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfn4rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
